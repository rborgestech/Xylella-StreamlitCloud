# -*- coding: utf-8 -*-
"""
core_xylella.py â€” versÃ£o hÃ­brida otimizada
MantÃ©m todas as funcionalidades originais (OCR Azure + Parser Colab + Excel Writer)
e adiciona:
 - Cache local de OCR (reutiliza resultados)
 - Processamento paralelo de requisiÃ§Ãµes
"""

import os
import re
import time
import json
import hashlib
import tempfile
import requests
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font, Alignment

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DiretÃ³rios e configuraÃ§Ã£o
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUTPUT_DIR = Path(os.getenv("OUTPUT_DIR", "output_final"))
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
OCR_CACHE_DIR = OUTPUT_DIR / "ocr_cache"
OCR_CACHE_DIR.mkdir(parents=True, exist_ok=True)
MAX_REQ_WORKERS = int(os.getenv("MAX_REQ_WORKERS", "4"))

BASE_DIR = Path(__file__).resolve().parent
TEMPLATE_PATH = Path(os.environ.get("TEMPLATE_PATH", BASE_DIR / "TEMPLATE_PXf_SGSLABIP1056.xlsx"))

AZURE_API_KEY = os.getenv("AZURE_API_KEY", "")
AZURE_ENDPOINT = os.getenv("AZURE_ENDPOINT", "")
MODEL_ID = os.getenv("AZURE_MODEL_ID", "prebuilt-document")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Cache OCR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _file_hash(path: str) -> str:
    h = hashlib.sha1()
    with open(path, "rb") as f:
        while chunk := f.read(1024 * 1024):
            h.update(chunk)
    return h.hexdigest()

def _cache_path(pdf_path: str) -> Path:
    return OCR_CACHE_DIR / f"{os.path.basename(pdf_path)}.{_file_hash(pdf_path)}.json"

def _load_cache(pdf_path: str):
    cp = _cache_path(pdf_path)
    if cp.exists():
        try:
            with open(cp, "r", encoding="utf-8") as f:
                data = json.load(f)
            print(f"â™»ï¸ OCR reutilizado ({cp.name})")
            return data
        except Exception:
            pass
    return None

def _save_cache(pdf_path: str, data: Dict[str, Any]):
    cp = _cache_path(pdf_path)
    try:
        with open(cp, "w", encoding="utf-8") as f:
            json.dump(data, f)
    except Exception:
        pass

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OCR Azure
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def azure_analyze_pdf(pdf_path: str) -> Dict[str, Any]:
    if not AZURE_API_KEY or not AZURE_ENDPOINT:
        raise RuntimeError("Azure nÃ£o configurado (AZURE_API_KEY/AZURE_ENDPOINT).")

    url = f"{AZURE_ENDPOINT.rstrip('/')}/formrecognizer/documentModels/{MODEL_ID}:analyze?api-version=2023-07-31"
    headers = {"Ocp-Apim-Subscription-Key": AZURE_API_KEY, "Content-Type": "application/pdf"}

    with open(pdf_path, "rb") as f:
        resp = requests.post(url, data=f.read(), headers=headers, timeout=120)
    if resp.status_code != 202:
        raise RuntimeError(f"Azure analyze falhou: {resp.status_code} {resp.text}")

    op = resp.headers.get("Operation-Location")
    if not op:
        raise RuntimeError("Azure nÃ£o devolveu Operation-Location.")

    start = time.time()
    while True:
        r = requests.get(op, headers={"Ocp-Apim-Subscription-Key": AZURE_API_KEY}, timeout=60)
        j = r.json()
        if j.get("status") == "succeeded":
            return j
        if j.get("status") == "failed":
            raise RuntimeError(f"OCR Azure falhou: {j}")
        if time.time() - start > 180:
            raise RuntimeError("Timeout a aguardar OCR Azure.")
        time.sleep(1.2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helpers de texto e datas
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_all_text(result_json: Dict[str, Any]) -> str:
    lines = []
    for pg in result_json.get("analyzeResult", {}).get("pages", []):
        for ln in pg.get("lines", []):
            txt = (ln.get("content") or ln.get("text") or "").strip()
            if txt:
                lines.append(txt)
    return "\n".join(lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Parser original completo
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âš ï¸ AQUI entra o teu parser completo da versÃ£o anterior
# (copiado integralmente)
from core_xylella import (
    detect_requisicoes,
    split_if_multiple_requisicoes,
    extract_context_from_text,
    parse_xylella_tables,
    write_to_template,
)

def parse_all_requisitions(result_json, pdf_name, txt_path):
    # IdÃªntico Ã  tua versÃ£o original (usa as funÃ§Ãµes importadas)
    if txt_path and os.path.exists(txt_path):
        full_text = Path(txt_path).read_text(encoding="utf-8")
    else:
        full_text = extract_all_text(result_json)

    count, _ = detect_requisicoes(full_text)
    all_tables = result_json.get("analyzeResult", {}).get("tables", []) or []

    if count <= 1:
        context = extract_context_from_text(full_text)
        amostras = parse_xylella_tables(result_json, context, req_id=1)
        expected = context.get("declared_samples", 0)
        return [{"rows": amostras, "expected": expected}] if amostras else []

    blocos = split_if_multiple_requisicoes(full_text)
    num_blocos = len(blocos)
    results = []
    for bi, bloco in enumerate(blocos, start=1):
        context = extract_context_from_text(bloco)
        amostras = parse_xylella_tables({"analyzeResult": {"tables": all_tables}}, context, req_id=bi)
        expected = context.get("declared_samples", 0)
        results.append({"rows": amostras, "expected": expected})
    return results

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FunÃ§Ã£o principal (com cache e paralelismo)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_pdf_sync(pdf_path: str) -> List[str]:
    base = os.path.basename(pdf_path)
    print(f"\nğŸ§ª InÃ­cio de processamento: {base}")
    txt_path = OUTPUT_DIR / f"{os.path.splitext(base)[0]}_ocr_debug.txt"

    # 1ï¸âƒ£ OCR com cache
    result_json = _load_cache(pdf_path)
    if result_json is None:
        result_json = azure_analyze_pdf(pdf_path)
        _save_cache(pdf_path, result_json)

    # 2ï¸âƒ£ Guardar texto OCR
    txt_path.write_text(extract_all_text(result_json), encoding="utf-8")
    print(f"ğŸ“ Texto OCR guardado em: {txt_path}")

    # 3ï¸âƒ£ Parsing
    requisitions = parse_all_requisitions(result_json, pdf_path, str(txt_path))
    if not requisitions:
        print("âš ï¸ Nenhuma requisiÃ§Ã£o encontrada.")
        return []

    # 4ï¸âƒ£ Paralelismo para escrita
    created_files = []
    with ThreadPoolExecutor(max_workers=min(MAX_REQ_WORKERS, len(requisitions))) as executor:
        futures = {
            executor.submit(
                write_to_template,
                req["rows"],
                f"{os.path.splitext(base)[0]}_req{i}.xlsx" if len(requisitions) > 1 else f"{os.path.splitext(base)[0]}.xlsx",
                req.get("expected"),
                pdf_path,
            ): i
            for i, req in enumerate(requisitions, start=1)
        }
        for fut in as_completed(futures):
            try:
                path = fut.result()
                if path:
                    created_files.append(path)
            except Exception as e:
                print(f"âŒ Erro ao gravar: {e}")

    print(f"ğŸ {base}: {len(created_files)} ficheiro(s) Excel gerado(s).")
    return created_files
