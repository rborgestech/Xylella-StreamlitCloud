# -*- coding: utf-8 -*-
"""
core_xylella.py ‚Äî Cloud/Streamlit (OCR Azure + Parser Colab + Writer por requisi√ß√£o)

API esperada pela UI (xylella_processor.py):
    ‚Ä¢ process_pdf_sync(pdf_path) -> List[List[Dict]]   # devolve listas de amostras por requisi√ß√£o
    ‚Ä¢ write_to_template(rows_per_req, out_base_path, expected_count=None, source_pdf=None) -> List[str]

Requer:
  - AZURE_API_KEY, AZURE_ENDPOINT (env)
  - TEMPLATE_PATH (env) ou ficheiro 'TEMPLATE_PXf_SGSLABIP1056.xlsx' ao lado do core
  - OUTPUT_DIR (env) ‚Äî diret√≥rio onde guardar .xlsx e _ocr_debug.txt
"""

from __future__ import annotations
import os, re, io, time, json, shutil, requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple

from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font, Alignment

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Caminhos / Ambiente
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
BASE_DIR = Path(__file__).parent
OUTPUT_DIR = Path(os.environ.get("OUTPUT_DIR", BASE_DIR / "Output"))
OUTPUT_DIR.mkdir(exist_ok=True)

TEMPLATE_PATH = Path(os.environ.get("TEMPLATE_PATH", BASE_DIR / "TEMPLATE_PXf_SGSLABIP1056.xlsx"))
if not TEMPLATE_PATH.exists():
    # n√£o falha j√° ‚Äî o app.py normalmente garante isto; fazemos s√≥ aviso
    print(f"‚ÑπÔ∏è Aviso: TEMPLATE n√£o encontrado em {TEMPLATE_PATH}. Ser√° verificado no momento da escrita.")

AZURE_API_KEY = os.environ.get("AZURE_API_KEY", "")
AZURE_ENDPOINT = os.environ.get("AZURE_ENDPOINT", "")
MODEL_ID = os.environ.get("AZURE_MODEL_ID", "prebuilt-document")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Estilos Excel
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
GREEN = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
RED   = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
GRAY  = PatternFill(start_color="E7E6E6", end_color="E7E6E6", fill_type="solid")
BOLD  = Font(bold=True, color="000000")
ITALIC= Font(italic=True, color="555555")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Utilit√°rios
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _is_valid_date(v) -> bool:
    try:
        datetime.strptime(str(v).strip(), "%d/%m/%Y")
        return True
    except Exception:
        return False

def _to_dt(v):
    try:
        return datetime.strptime(str(v).strip(), "%d/%m/%Y")
    except Exception:
        return v

def extract_all_text(result_json: Dict[str, Any]) -> str:
    lines = []
    for pg in result_json.get("analyzeResult", {}).get("pages", []):
        for ln in pg.get("lines", []):
            txt = (ln.get("content") or ln.get("text") or "").strip()
            if txt:
                lines.append(txt)
    return "\n".join(lines)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# OCR Azure (PDF direto)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def azure_analyze_pdf(pdf_path: str) -> Dict[str, Any]:
    if not AZURE_API_KEY or not AZURE_ENDPOINT:
        raise RuntimeError("Azure n√£o configurado (AZURE_API_KEY/AZURE_ENDPOINT).")

    url = f"{AZURE_ENDPOINT.rstrip('/')}/formrecognizer/documentModels/{MODEL_ID}:analyze?api-version=2023-07-31"
    headers = {"Ocp-Apim-Subscription-Key": AZURE_API_KEY, "Content-Type": "application/pdf"}

    with open(pdf_path, "rb") as f:
        resp = requests.post(url, data=f.read(), headers=headers, timeout=90)
    if resp.status_code != 202:
        raise RuntimeError(f"Azure analyze falhou: {resp.status_code} {resp.text}")

    op = resp.headers.get("Operation-Location")
    if not op:
        raise RuntimeError("Azure n√£o devolveu Operation-Location.")

    for _ in range(60):
        time.sleep(1.2)
        r = requests.get(op, headers={"Ocp-Apim-Subscription-Key": AZURE_API_KEY}, timeout=30)
        j = r.json()
        if j.get("status") == "succeeded":
            return j
        if j.get("status") == "failed":
            raise RuntimeError(f"OCR Azure falhou: {j}")
    raise RuntimeError("Timeout a aguardar OCR Azure.")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Parser ‚Äî Sec√ß√µes do Colab (adaptadas)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NATUREZA_KEYWORDS = [
    "ramos","folhas","ramosefolhas","ramosc/folhas","material","materialherbalho",
    "materialherb√°rio","materialherbalo","natureza","insetos","sementes","solo"
]
TIPO_RE = re.compile(r"\b(Simples|Composta|Individual)\b", re.I)

def _looks_like_natureza(txt: str) -> bool:
    t = re.sub(r"\s+", "", (txt or "").lower())
    return any(k in t for k in NATUREZA_KEYWORDS)

def _clean_ref(raw: str) -> str:
    s = (raw or "").strip()
    s = re.sub(r"\s*/\s*", "/", s)
    s = re.sub(r"/{2,}", "/", s)
    s = re.sub(r"[A-Za-z]+", lambda m: m.group(0).upper(), s)
    s = s.replace("LUT", "LVT")
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"[^A-Z0-9/]+$", "", s)
    return s

def detect_requisicoes(full_text: str):
    """Conta quantas requisi√ß√µes DGAV‚ÜíSGS existem no texto OCR de um PDF."""
    pattern = re.compile(
        r"PROGRAMA\s+NACIONAL\s+DE\s+PROSPE[√áC][A√É]O\s+DE\s+PRAGAS\s+DE\s+QUARENTENA",
        re.IGNORECASE,
    )
    matches = list(pattern.finditer(full_text))
    count = len(matches)
    positions = [m.start() for m in matches]
    if count == 0:
        print("üîç Nenhum cabe√ßalho encontrado ‚Äî assumido 1 requisi√ß√£o.")
        count = 1
    else:
        print(f"üîç Detetadas {count} requisi√ß√µes no ficheiro (posi√ß√µes: {positions})")
    return count, positions


def split_if_multiple_requisicoes(full_text: str):
    """
    Divide o texto OCR em blocos distintos (requisi√ß√µes DGAV‚ÜíSGS),
    tolerando erros e quebras do OCR.
    """
    # Normaliza o texto (remove m√∫ltiplos espa√ßos e capitaliza)
    text = re.sub(r"\s+", " ", full_text, flags=re.M).upper()

    # Padr√£o flex√≠vel: aceita varia√ß√µes e pequenas falhas no OCR
    pattern = re.compile(
        r"P\s*R\s*O\s*G\s*R\s*A\s*M\s*A\s+N\s*A\s*C\s*I\s*O\s*N\s*A\s*L\s+DE\s+PROSPE[C√á]\s*AO\s+DE\s+PRAGAS\s+DE\s+QUARENTENA",
        re.IGNORECASE,
    )

    # Encontrar todas as ocorr√™ncias
    marks = [m.start() for m in pattern.finditer(text)]
    if not marks:
        print("üîç Nenhum cabe√ßalho encontrado ‚Äî tratado como 1 requisi√ß√£o.")
        return [full_text]
    if len(marks) == 1:
        print("üîç Apenas 1 cabe√ßalho ‚Äî 1 requisi√ß√£o detectada.")
        return [full_text]

    # Adiciona o fim do texto como limite final
    marks.append(len(text))
    blocos = []

    for i in range(len(marks) - 1):
        start = max(0, marks[i] - 200)           # inclui parte do cabe√ßalho anterior
        end = min(len(text), marks[i + 1] + 200)
        bloco = text[start:end].strip()
        if len(bloco) > 300:
            blocos.append(bloco)
        else:
            print(f"‚ö†Ô∏è Bloco {i+1} demasiado pequeno ({len(bloco)} chars).")

    print(f"üîç Detetadas {len(blocos)} requisi√ß√µes distintas (por cabe√ßalho).")
    return blocos


def extract_context_from_text(full_text: str):
    """Extrai informa√ß√µes gerais da requisi√ß√£o (zona, DGAV, datas, n¬∫ de amostras)."""
    ctx = {}

    # Zona
    m_zona = re.search(r"Xylella\s+fastidiosa\s*\(([^)]+)\)", full_text, re.I)
    ctx["zona"] = m_zona.group(1).strip() if m_zona else "Zona Isenta"

    # DGAV / Respons√°vel
    responsavel, dgav = None, None
    m_hdr = re.search(
        r"Amostra(?:s|\(s\))?\s*colhida(?:s|\(s\))?\s*por\s*DGAV\s*[:\-]?\s*(.*)",
        full_text, re.IGNORECASE,
    )
    if m_hdr:
        tail = full_text[m_hdr.end():]
        linhas = [m_hdr.group(1)] + tail.splitlines()
        for ln in linhas[:4]:
            ln = (ln or "").strip()
            if ln:
                responsavel = ln
                break
        if responsavel:
            responsavel = re.sub(r"\S+@dgav\.pt|\S+@\S+", "", responsavel, flags=re.I)
            responsavel = re.sub(r"PROGRAMA.*|Data.*|N[¬∫¬∞].*", "", responsavel, flags=re.I)
            responsavel = re.sub(r"[:;,.\-‚Äì‚Äî]+$", "", responsavel).strip()

    if responsavel:
        dgav = f"DGAV {responsavel}".strip() if not re.match(r"^DGAV\b", responsavel, re.I) else responsavel
    else:
        m_d = re.search(r"\bDGAV(?:\s+[A-Za-z√Ä-√ø?]+){1,4}", full_text)
        dgav = re.sub(r"[:;,.\-‚Äì‚Äî]+$", "", m_d.group(0)).strip() if m_d else None

    ctx["dgav"] = dgav
    ctx["responsavel_colheita"] = None

    # Datas de colheita
    colheita_map = {}
    for m in re.finditer(r"(\d{1,2}/\d{1,2}/\d{4})\s*\(\s*(\*+)\s*\)", full_text):
        colheita_map[f"({m.group(2).replace(' ', '')})"] = m.group(1)
    if not colheita_map:
        m_simple = re.search(r"Data\s+de\s+colheita\s*[:\-\s]*([0-9/\-\s]+)", full_text, re.I)
        if m_simple:
            only_date = re.sub(r"\s+", "", m_simple.group(1))
            for key in ("(*)", "(**)", "(***)"):
                colheita_map[key] = only_date
    default_colheita = next(iter(colheita_map.values()), "")
    ctx["colheita_map"] = colheita_map
    ctx["default_colheita"] = default_colheita

    # Data de envio
    m_envio = re.search(
        r"Data\s+(?:do|de)\s+envio(?:\s+ao\s+laborat[o√≥]rio)?[:\-\s]*([0-9/\-\s]+)",
        full_text, re.I,
    )
    if m_envio:
        ctx["data_envio"] = re.sub(r"\s+", "", m_envio.group(1))
    elif default_colheita:
        ctx["data_envio"] = default_colheita
    else:
        ctx["data_envio"] = datetime.now().strftime("%d/%m/%Y")

    # N¬∫ de amostras declaradas
    flat = re.sub(r"\s+", " ", full_text)
    m_decl = re.search(r"N[¬∫¬∞]?\s*de\s*amostras(?:\s+neste\s+envio)?\s*[:\-]?\s*(\d{1,4})", flat, re.I)
    ctx["declared_samples"] = int(m_decl.group(1)) if m_decl else None

    return ctx

def parse_xylella_tables(result_json, context, req_id=None):
    """Extrai as amostras das tabelas Azure OCR, aplicando o contexto da requisi√ß√£o."""
    out = []
    tables = result_json.get("analyzeResult", {}).get("tables", [])
    if not tables:
        print("‚ö†Ô∏è Nenhuma tabela encontrada.")
        return out

    for t in tables:
        nc = max(c.get("columnIndex", 0) for c in t.get("cells", [])) + 1
        nr = max(c.get("rowIndex", 0) for c in t.get("cells", [])) + 1
        grid = [[""] * nc for _ in range(nr)]
        for c in t.get("cells", []):
            grid[c["rowIndex"]][c["columnIndex"]] = clean_value(c.get("content", ""))

        for row in grid:
            if not row or not any(row):
                continue
            ref = _clean_ref(row[0]) if len(row) > 0 else ""
            if not ref or re.match(r"^\D+$", ref):
                continue

            hospedeiro = row[2] if len(row) > 2 else ""
            obs        = row[3] if len(row) > 3 else ""

            if _looks_like_natureza(hospedeiro):
                hospedeiro = ""

            tipo = ""
            joined = " ".join([x for x in row if isinstance(x, str)])
            m_tipo = re.search(r"\b(Simples|Composta|Individual)\b", joined, re.I)
            if m_tipo:
                tipo = m_tipo.group(1).capitalize()
                obs = re.sub(r"\b(Simples|Composta|Individual)\b", "", obs, flags=re.I).strip()

            datacolheita = context.get("default_colheita", "")
            m_ast = re.search(r"\(\s*\*+\s*\)", joined)
            if m_ast:
                mark = re.sub(r"\s+", "", m_ast.group(0))
                datacolheita = context.get("colheita_map", {}).get(mark, datacolheita)

            if obs.strip().lower() in ("simples", "composta", "individual"):
                obs = ""

            out.append({
                "requisicao_id": req_id,
                "datarececao": context["data_envio"],
                "datacolheita": datacolheita,
                "referencia": ref,
                "hospedeiro": hospedeiro,
                "tipo": tipo,
                "zona": context["zona"],
                "responsavelamostra": context["dgav"],
                "responsavelcolheita": context["responsavel_colheita"],
                "observacoes": obs.strip(),
                "procedure": "XYLELLA",
                "datarequerido": context["data_envio"],
                "Score": ""
            })

    print(f"‚úÖ {len(out)} amostras extra√≠das no total (req_id={req_id}).")
    return out
    
def parse_xylella_from_result(result_json, pdf_name, txt_path=None):
    """
    Analisa o resultado OCR e extrai todas as requisi√ß√µes de um PDF Xylella.
    Divide o documento em blocos e associa as tabelas a cada bloco.
    """
    import os

    print(f"üìÑ A processar: {pdf_name}")

    # 1Ô∏è‚É£ Ler texto global do OCR
    if txt_path and os.path.exists(txt_path):
        with open(txt_path, "r", encoding="utf-8") as f:
            full_text = f.read()
        print(f"üìù Contexto extra√≠do de {os.path.basename(txt_path)}")
    else:
        print("‚ö†Ô∏è Ficheiro de texto global n√£o encontrado ‚Äî fallback: 1.¬™ p√°gina.")
        first_page_text = "\n".join(
            line.get("content", "")
            for line in result_json.get("analyzeResult", {}).get("pages", [])[0].get("lines", [])
        )
        full_text = first_page_text

    # 2Ô∏è‚É£ Detetar requisi√ß√µes
    count, _ = detect_requisicoes(full_text)

    # 3Ô∏è‚É£ Documento com apenas uma requisi√ß√£o
    if count <= 1:
        print("üìÑ Documento cont√©m apenas uma requisi√ß√£o.")
        context = extract_context_from_text(full_text)
        amostras = parse_xylella_tables(result_json, context, req_id=1)

        if not amostras:
            append_process_log(pdf_name, 1, 0, context.get("declared_samples"),
                               out_path=None, status="Vazia",
                               error_msg="Sem amostras v√°lidas.")
            return []

        out_path = write_to_template(amostras, os.path.basename(pdf_name),
                                     expected_count=context.get("declared_samples"),
                                     source_pdf=pdf_name)
        append_process_log(pdf_name, 1, len(amostras),
                           context.get("declared_samples"), out_path=out_path, status="OK")
        return amostras

    # 4Ô∏è‚É£ Documento com m√∫ltiplas requisi√ß√µes
    blocos = split_if_multiple_requisicoes(full_text)
    print(f"üìÑ Documento dividido em {len(blocos)} requisi√ß√µes.")

    all_samples = []
    all_tables = result_json.get("analyzeResult", {}).get("tables", [])

    for i, bloco in enumerate(blocos, start=1):
        print(f"\nüîπ A processar requisi√ß√£o {i}/{len(blocos)}...")

        try:
            bloco = re.sub(r"[ \t]+", " ", bloco.replace("\r", ""))
            context = extract_context_from_text(bloco)

            refs_bloco = re.findall(
                r"\b\d{1,3}/[A-Z]{0,2}/DGAV(?:-[A-Z0-9/]+)?|\b\d{2,4}/\d{2,4}/[A-Z0-9\-]+",
                bloco, re.I
            )
            refs_bloco = [r.strip() for r in refs_bloco if len(r.strip()) > 4]
            print(f"   ‚Ü≥ {len(refs_bloco)} refer√™ncias detetadas no bloco {i}")

            tables_filtradas = [
                t for t in all_tables
                if any(ref in " ".join(c.get("content", "") for c in t.get("cells", []))
                       for ref in refs_bloco)
            ] or all_tables

            result_local = {"analyzeResult": {"tables": tables_filtradas}}
            amostras = parse_xylella_tables(result_local, context, req_id=i)

            if not amostras:
                append_process_log(pdf_name, i, 0, context.get("declared_samples"),
                                   out_path=None, status="Vazia",
                                   error_msg="Sem amostras v√°lidas (OCR incompleto).")
                continue

            base = os.path.splitext(os.path.basename(pdf_name))[0]
            out_name = f"{base}_req{i}.xlsx"
            out_path = write_to_template(amostras, out_name,
                                         expected_count=context.get("declared_samples"),
                                         source_pdf=pdf_name)
            append_process_log(pdf_name, i, len(amostras),
                               context.get("declared_samples"), out_path=out_path, status="OK")
            print(f"‚úÖ Requisi√ß√£o {i} gravada em: {out_path}")
            all_samples.extend(amostras)

        except Exception as e:
            print(f"‚ùå Erro na requisi√ß√£o {i}: {e}")
            append_process_log(pdf_name, i, 0, None,
                               out_path=None, status="Erro", error_msg=str(e))

    print(f"\nüèÅ Conclu√≠do: {len(blocos)} requisi√ß√µes processadas, {len(all_samples)} amostras no total.")
    return all_samples
    
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Parser: dividir e extrair requisi√ß√µes
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def parse_all_requisitions(result_json: Dict[str, Any], pdf_name: str, txt_path: str | None) -> List[List[Dict[str, Any]]]:
    """Divide o documento em blocos (requisi√ß√µes) e extrai amostras por bloco."""
    # Texto global do OCR
    if txt_path and os.path.exists(txt_path):
        full_text = Path(txt_path).read_text(encoding="utf-8")
        print(f"üìù Contexto extra√≠do de {os.path.basename(txt_path)}")
    else:
        full_text = extract_all_text(result_json)
        print("‚ö†Ô∏è Ficheiro OCR n√£o encontrado ‚Äî a usar texto direto do OCR.")

    # Dete√ß√£o de requisi√ß√µes (cabe√ßalhos)
    count, _ = detect_requisicoes(full_text)
    if count == 0:
        print("‚ö†Ô∏è Nenhum cabe√ßalho detectado ‚Äî assumido 1 requisi√ß√£o.")
        count = 1

    all_tables = result_json.get("analyzeResult", {}).get("tables", []) or []
    out: List[List[Dict[str, Any]]] = []

    # Documento simples (1 requisi√ß√£o)
    if count <= 1:
        context = extract_context_from_text(full_text)
        amostras = parse_xylella_tables(result_json, context, req_id=1)
        return [amostras] if amostras else []

    # Documento com m√∫ltiplas requisi√ß√µes
    blocos = split_if_multiple_requisicoes(full_text)
    print(f"üìÑ Documento dividido em {len(blocos)} requisi√ß√µes distintas.")

    for i, bloco in enumerate(blocos, start=1):
        try:
            context = extract_context_from_text(bloco)
            refs_bloco = re.findall(r"\b\d{7,8}\b|\b\d{2,4}/\d{2,4}/[A-Z0-9\-]+\b", bloco, re.I)

            # Filtra tabelas correspondentes a esta requisi√ß√£o
            tables_filtradas = []
            for t in all_tables:
                joined = " ".join(c.get("content", "") for c in t.get("cells", []))
                if any(ref in joined for ref in refs_bloco):
                    tables_filtradas.append(t)

            if not tables_filtradas and i == 1:
                print("‚ö†Ô∏è Nenhuma tabela filtrada ‚Äî usar todas por seguran√ßa.")
                tables_filtradas = all_tables

            local = {"analyzeResult": {"tables": tables_filtradas}}
            amostras = parse_xylella_tables(local, context, req_id=i)

            if amostras:
                print(f"‚úÖ Requisi√ß√£o {i}: {len(amostras)} amostras.")
                out.append(amostras)
            else:
                print(f"‚ö†Ô∏è Requisi√ß√£o {i} sem amostras extra√≠das.")

        except Exception as e:
            print(f"‚ùå Erro na requisi√ß√£o {i}: {e}")

    return out

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Escrita no TEMPLATE ‚Äî 1 ficheiro por requisi√ß√£o
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def to_datetime(value: str):
    """Converte string dd/mm/yyyy para objeto datetime (ou devolve None se inv√°lida)."""
    try:
        return datetime.strptime(str(value).strip(), "%d/%m/%Y")
    except Exception:
        return None
        
def write_to_template(ocr_rows, out_name, expected_count=None, source_pdf=None):
    """
    Escreve as linhas extra√≠das no template base.
    Cria um ficheiro Excel no diret√≥rio OUTPUT_DIR com o nome indicado.
    Campo de observa√ß√µes (coluna I) √© sempre deixado vazio.
    Inclui:
      ‚Ä¢ Valida√ß√£o do n¬∫ de amostras (E1:F1)
      ‚Ä¢ Origem real do PDF (G1:J1)
      ‚Ä¢ Data/hora de processamento (K1:L1)
      ‚Ä¢ Convers√£o autom√°tica de datas
      ‚Ä¢ Valida√ß√£o de campos obrigat√≥rios
      ‚Ä¢ F√≥rmula Data requerido = Data Rece√ß√£o + 30 dias
    """
    from openpyxl import load_workbook
    from openpyxl.styles import PatternFill, Font, Alignment
    from datetime import datetime
    import os

    if not ocr_rows:
        print(f"‚ö†Ô∏è {out_name}: sem linhas para escrever.")
        return None

    if not os.path.exists(TEMPLATE_PATH):
        raise FileNotFoundError(f"Template n√£o encontrado: {TEMPLATE_PATH}")

    wb = load_workbook(TEMPLATE_PATH)
    ws = wb.worksheets[0]
    start_row = 4

    # üé® Estilos
    yellow_fill = PatternFill(start_color="FFFACD", end_color="FFFACD", fill_type="solid")
    green_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
    red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
    gray_fill = PatternFill(start_color="E7E6E6", end_color="E7E6E6", fill_type="solid")
    bold_center = Font(bold=True, color="000000")

    def is_valid_date(value: str) -> bool:
        try:
            if isinstance(value, datetime):
                return True
            datetime.strptime(str(value).strip(), "%d/%m/%Y")
            return True
        except Exception:
            return False

    def to_datetime(value: str):
        try:
            return datetime.strptime(str(value).strip(), "%d/%m/%Y")
        except Exception:
            return None

    # üßπ Limpar linhas anteriores (mant√©m apenas cabe√ßalhos)
    for row in range(start_row, 201):
        for col in range(1, 13):
            ws.cell(row=row, column=col).value = None
            ws.cell(row=row, column=col).fill = PatternFill(fill_type=None)

    # ‚úçÔ∏è Escrever novas linhas
    for idx, row in enumerate(ocr_rows, start=start_row):
        rececao_val = row.get("datarececao", "")
        colheita_val = row.get("datacolheita", "")

        cell_A = ws[f"A{idx}"]
        cell_B = ws[f"B{idx}"]

        if is_valid_date(rececao_val):
            cell_A.value = to_datetime(rececao_val)
        else:
            cell_A.value = rececao_val
            cell_A.fill = red_fill

        if is_valid_date(colheita_val):
            cell_B.value = to_datetime(colheita_val)
        else:
            cell_B.value = colheita_val
            cell_B.fill = red_fill

        ws[f"C{idx}"] = row.get("referencia", "")
        ws[f"D{idx}"] = row.get("hospedeiro", "")
        ws[f"E{idx}"] = row.get("tipo", "")
        ws[f"F{idx}"] = row.get("zona", "")
        ws[f"G{idx}"] = row.get("responsavelamostra", "")
        ws[f"H{idx}"] = row.get("responsavelcolheita", "")
        ws[f"I{idx}"] = ""  # Observa√ß√µes
        ws[f"K{idx}"] = row.get("procedure", "")
        ws[f"L{idx}"] = f"=A{idx}+30"  # Data requerido = Data rece√ß√£o + 30 dias

        # Campos obrigat√≥rios
        for col in ["A", "B", "C", "D", "E", "F", "G"]:
            cell = ws[f"{col}{idx}"]
            if not cell.value or str(cell.value).strip() == "":
                cell.fill = red_fill

        # Destaque amarelo (revis√£o)
        if row.get("WasCorrected") or row.get("ValidationStatus") in ("review", "unknown", "no_list"):
            ws[f"D{idx}"].fill = yellow_fill

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üìä Valida√ß√£o E1:F1 ‚Äî N¬∫ Amostras declaradas / processadas
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    processed = len(ocr_rows)
    expected = expected_count if expected_count is not None else "?"
    ws.merge_cells("E1:F1")
    cell = ws["E1"]
    cell.value = f"N¬∫ Amostras: {expected} / {processed}"
    cell.font = bold_center
    cell.alignment = Alignment(horizontal="center", vertical="center")
    cell.fill = green_fill if expected == processed else red_fill

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üóÇÔ∏è Origem do PDF
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ws.merge_cells("G1:J1")
    pdf_orig_name = os.path.basename(source_pdf) if source_pdf else "(desconhecida)"
    ws["G1"].value = f"Origem: {pdf_orig_name}"
    ws["G1"].font = Font(italic=True, color="555555")
    ws["G1"].alignment = Alignment(horizontal="left", vertical="center")
    ws["G1"].fill = gray_fill

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üïí Data/hora de processamento
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ws.merge_cells("K1:L1")
    timestamp = datetime.now().strftime("%d/%m/%Y %H:%M")
    ws["K1"].value = f"Processado em: {timestamp}"
    ws["K1"].font = Font(italic=True, color="555555")
    ws["K1"].alignment = Alignment(horizontal="right", vertical="center")
    ws["K1"].fill = gray_fill

    # üíæ Guardar ficheiro
    base_name = os.path.splitext(os.path.basename(out_name))[0]
    out_path = os.path.join(OUTPUT_DIR, f"{base_name}.xlsx")
    wb.save(out_path)
    print(f"üü¢ Gravado com sucesso: {out_path}")

    return out_path

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# OCR + Parsing (devolve listas por requisi√ß√£o)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 6. UTILIT√ÅRIOS GERAIS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def clean_value(s: str) -> str:
    """Limpa e normaliza um valor OCR."""
    if s is None:
        return ""
    if isinstance(s, (int, float)):
        return str(s)
    s = re.sub(r"[\u200b\t\r\f\v]+", " ", s)
    s = (s.strip()
           .replace("N/A", "")
           .replace("%", "")
           .replace("\n", " ")
           .replace("  ", " "))
    return s.strip()

def to_datetime(value: str):
    try:
        return datetime.strptime(str(value).strip(), "%d/%m/%Y")
    except Exception:
        return None

def pdf_to_images(pdf_path):
    """Converte PDF em imagens."""
    return convert_from_path(pdf_path, dpi=150)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 11. FUN√á√ïES ASS√çNCRONAS DE OCR (Azure)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def azure_ocr_page(session, img_bytes, page_idx, pdf_name, cache_dir):
    """Envia imagem para OCR Azure (com cache e ignorando p√°ginas vazias)."""
    cache_file = os.path.join(cache_dir, f"{os.path.basename(pdf_name)}_p{page_idx}.json")

    # Cache: evita chamadas repetidas
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            result_json = json.load(f)
        print(f"üì¶ OCR cache usado (p√°gina {page_idx})")
    else:
        url = f"{AZURE_ENDPOINT}formrecognizer/documentModels/{MODEL_ID}:analyze?api-version=2023-07-31"
        headers = {"Ocp-Apim-Subscription-Key": AZURE_API_KEY, "Content-Type": "application/octet-stream"}

        async with session.post(url, data=img_bytes, headers=headers) as resp:
            if resp.status != 202:
                txt = await resp.text()
                print(f"‚ùå Erro Azure ({page_idx}): {txt}")
                return None
            result_url = resp.headers.get("Operation-Location")

        # Polling at√© o OCR estar conclu√≠do
        for _ in range(20):
            await asyncio.sleep(2)
            async with session.get(result_url, headers={"Ocp-Apim-Subscription-Key": AZURE_API_KEY}) as r:
                j = await r.json()
                if j.get("status") == "succeeded":
                    result_json = j
                    break
        else:
            print(f"‚ö†Ô∏è Timeout OCR p√°gina {page_idx}")
            return None

        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(result_json, f, ensure_ascii=False)

    # Ignorar p√°ginas em branco
    text = extract_all_text(result_json)
    if len(text.strip()) < 100:
        return None

    tables = result_json.get("analyzeResult", {}).get("tables", [])
    return (page_idx, text, tables)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  OCR + PARSING COMPLETO PARA UM PDF
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def process_pdf_async(pdf_path, session):
    print(f"\nüìÑ A processar async: {os.path.basename(pdf_path)}")
    t0 = asyncio.get_event_loop().time()

    cache_dir = os.path.join(OUTPUT_DIR, "_ocr_cache")
    os.makedirs(cache_dir, exist_ok=True)

    # Converter PDF em imagens (thread pool)
    with ThreadPoolExecutor() as pool:
        images = await asyncio.get_event_loop().run_in_executor(pool, lambda: pdf_to_images(pdf_path))

    # OCR paralelo das p√°ginas
    tasks = []
    for i, img in enumerate(images, start=1):
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        img_bytes = buf.getvalue()
        tasks.append(azure_ocr_page(session, img_bytes, i, pdf_path, cache_dir))

    results = await asyncio.gather(*tasks)
    results = [r for r in results if r]

    if not results:
        print("‚ö†Ô∏è Nenhuma p√°gina √∫til ap√≥s OCR.")
        return

    results.sort(key=lambda x: x[0])
    full_text = "\n".join([f"\n\n--- P√ÅGINA {i} ---\n{text}" for i, text, _ in results])
    all_tables = [t for _, _, tbls in results for t in tbls]

    t_ocr = asyncio.get_event_loop().time() - t0
    print(f"‚è±Ô∏è OCR paralelo total: {timedelta(seconds=round(t_ocr))}")

    # Guardar texto global
    base_name = os.path.splitext(os.path.basename(pdf_path))[0]
    txt_path = os.path.join(OUTPUT_DIR, base_name + "_ocr_debug.txt")
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(full_text)

    # Parsing normal (usa fun√ß√£o j√° existente)
    t_parse_start = asyncio.get_event_loop().time()
    combined_json = {"analyzeResult": {"tables": all_tables, "pages": []}}
    rows = parse_xylella_from_result(combined_json, pdf_path, txt_path)
    t_parse = asyncio.get_event_loop().time() - t_parse_start

    print(f"‚è±Ô∏è Parsing + gera√ß√£o Excel: {timedelta(seconds=round(t_parse))}")
    print(f"üèÅ Total: {timedelta(seconds=round(asyncio.get_event_loop().time() - t0))}")

    return rows


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  PROCESSAMENTO ASS√çNCRONO DE TODOS OS PDFs
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def process_folder_async(input_dir):
    """
    Processa todos os PDFs de forma ass√≠ncrona e gera um resumo final
    com tempos m√©dios, totais e n¬∫ de amostras extra√≠das.
    """
    pdfs = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.lower().endswith(".pdf")]
    if not pdfs:
        print("‚ÑπÔ∏è N√£o h√° PDFs na pasta de entrada.")
        return

    start_time = asyncio.get_event_loop().time()
    summary = []

    async with aiohttp.ClientSession() as session:
        tasks = [process_pdf_async(pdf, session) for pdf in pdfs]
        results = await asyncio.gather(*tasks)

    # Montar resumo de desempenho
    total_time = asyncio.get_event_loop().time() - start_time
    total_pdfs = len(pdfs)
    total_rows = 0
    total_time_ocr = 0
    total_time_parse = 0

    # Cada resultado √© o "rows" retornado por process_pdf_async
    for pdf, res in zip(pdfs, results):
        if not res:
            continue
        total_rows += len(res)

    avg_time_per_pdf = total_time / total_pdfs if total_pdfs else 0

    print("\nüìä Resumo Final")
    print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    print(f"üìÑ PDFs processados: {total_pdfs}")
    print(f"üßæ Total de amostras extra√≠das: {total_rows}")
    print(f"‚è±Ô∏è Tempo total: {timedelta(seconds=round(total_time))}")
    print(f"‚öôÔ∏è Tempo m√©dio por PDF: {timedelta(seconds=round(avg_time_per_pdf))}")
    print(f"üìÇ Sa√≠da: {OUTPUT_DIR}")
    print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# API p√∫blica ‚Äî processamento s√≠ncrono
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def process_pdf_sync(pdf_path: str):
    """
    Executa o OCR Azure e o parser Colab de forma s√≠ncrona.
    Devolve listas de amostras por requisi√ß√£o.
    """
    print(f"\nüß™ In√≠cio de processamento: {os.path.basename(pdf_path)}")

    # 1Ô∏è‚É£ OCR Azure
    result_json = azure_analyze_pdf(pdf_path)

    # 2Ô∏è‚É£ Gera texto OCR detalhado (para permitir dete√ß√£o de cabe√ßalhos)
    base = os.path.splitext(os.path.basename(pdf_path))[0]
    txt_path = OUTPUT_DIR / f"{base}_ocr_debug.txt"

    full_text = extract_all_text(result_json)
    txt_path.write_text(full_text, encoding="utf-8")

    if len(full_text) < 2000:
        print("‚ö†Ô∏è OCR curto ‚Äî pode n√£o conter todos os cabe√ßalhos. Verifica se o PDF tem imagens digitalizadas.")

    # 3Ô∏è‚É£ Parser completo (com dete√ß√£o de m√∫ltiplas requisi√ß√µes)
    rows_per_req = parse_all_requisitions(result_json, pdf_path, str(txt_path))

    # 4Ô∏è‚É£ Estat√≠sticas finais
    total_amostras = sum(len(r) for r in rows_per_req)
    print(f"‚úÖ {os.path.basename(pdf_path)}: {len(rows_per_req)} requisi√ß√µes, {total_amostras} amostras extra√≠das.")

    return rows_per_req

pass







